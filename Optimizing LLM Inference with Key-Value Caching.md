# **Optimizing LLM Inference with Key-Value Caching**

## **Introduction**

Large Language Model (LLM) inference involves two phases: a **prefill (prompt processing)** phase and a **decode (generation)** phase ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=Large%20Language%20Models%20,check%20out%20this%20blog%20post)). In the prefill phase, all input tokens (the prompt or context) are processed in parallel through the model’s layers to produce the initial state for generation. In the decode phase, the model generates output tokens one-by-one in an autoregressive fashion, each time conditioning on all tokens generated so far ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=Large%20Language%20Models%20,check%20out%20this%20blog%20post)). This process can be **computationally intensive and latency-sensitive**, especially as models and context lengths grow. Two key performance metrics for LLM serving are *Time To First Token (TTFT)* and *Time Per Output Token (TPOT)* ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=Our%20team%20uses%20four%20key,metrics%20for%20LLM%20serving)):

* **Time To First Token (TTFT):** The latency from receiving a query to producing the first token of output. TTFT is largely determined by how fast the model can process the prompt and generate the initial token ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=1,450%20words%20per)). Low TTFT is critical for interactive applications (e.g. chat), where a user expects a quick initial response.

* **Time Per Output Token (TPOT):** The average time to generate each subsequent token in the output stream ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=This%20metric%20is%20driven%20by,number%20of%20tokens%20to%20be)). TPOT (sometimes called inter-token latency ([Metrics — NVIDIA NIM LLMs Benchmarking](https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html#:~:text=Inter,TPOT))) directly affects throughput as perceived by the user – for example, a TPOT of 100 ms corresponds to 10 tokens/sec per user, roughly 450 words per minute ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=This%20metric%20is%20driven%20by,number%20of%20tokens%20to%20be)). In other terms, TPOT is the inverse of the model’s streaming token throughput for a single sequence.

Modern LLM deployments seek to optimize both **low latency (TTFT)** and **high throughput (low TPOT)** simultaneously ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Doubao,%EC%9D%98%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EB%AA%A9%ED%91%9C%EB%A5%BC%20%EB%8F%99%EC%8B%9C%EC%97%90%20%EB%8B%AC%EC%84%B1%ED%95%A9%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Decode%20%EB%8B%A8%EA%B3%84%3A%20%EA%B3%84%EC%82%B0%20%EB%B3%91%EB%AA%A9%20%ED%98%84%EC%83%81%EC%9D%80,%EC%A0%84%EB%9E%B5%EC%9D%84%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC%20TPOT%20%EC%A7%80%ED%91%9C%EB%A5%BC%20%EB%82%AE%EC%B6%A5%EB%8B%88%EB%8B%A4)). A major innovation enabling these optimizations is the use of a **Key-Value (KV) cache** during inference. This cache stores the intermediate attention keys and values from prior tokens, and reusing them dramatically accelerates the decode phase of generation ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=A%20highly%20effective%20method%20to,increases%2C%20storing%20the%20KV%20cache)). As we will examine, KV caching improves time-to-first-token and per-token throughput, but also introduces challenges in memory utilization. In this whitepaper, we provide an in-depth look at the benefits and optimizations of KV caching in LLM inference, covering: (1) how KV cache improves TTFT and TPOT; (2) efficient on-GPU memory usage and GPU utilization; (3) extending KV caching to CPU and even persistent storage (SSD/NVMe) with associated trade-offs; and (4) the evolution of KV cache strategies in large-scale systems. We draw on recent literature (e.g. *OSDI 2024*, *ICML 2024*), industry technical reports, and open-source projects to illustrate the state-of-the-art.

## **KV Cache Mechanics and Impact on Latency & Throughput**

During autoregressive generation, a transformer model must continually attend to earlier tokens in the sequence. Naively, at each new token generation step, the model would need to recompute the key and value representations of *all* previous tokens to perform self-attention – an expensive approach leading to redundant computation ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Recomputation%20reduction,59)). The **KV cache** avoids this redundancy by storing the key ($K$) and value ($V$) tensors for each past token (at each transformer layer) in memory after they are first computed during attention. When the model generates the next token, it can **reuse** these cached $K$ and $V$ vectors instead of recomputing them, needing only to compute the query ($Q$) for the new token and perform attention with the cached states ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=A%20highly%20effective%20method%20to,increases%2C%20storing%20the%20KV%20cache)). Essentially, caching transforms the attention workload for each new token from a full matrix-matrix multiplication over all prior tokens to a matrix-vector multiplication involving just the new token’s query and the stored keys/values ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=A%20highly%20effective%20method%20to,increases%2C%20storing%20the%20KV%20cache)). This yields a **dramatic reduction in per-token computation** – mathematically, the complexity per decoding step drops from $O(n^2)$ to $O(n)$ (where $n$ is sequence length) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=A%20highly%20effective%20method%20to,increases%2C%20storing%20the%20KV%20cache)). In practical terms, KV caching converts the decode phase from a *compute-bound* regime to much closer to a *memory-bound* regime (), since each step must fetch a growing history of keys/values but performs relatively small new computes.

**Time to First Token (TTFT):** Using KV caching can improve TTFT in scenarios with long prompts or when prompts are processed in parts. Typically, TTFT is dominated by the prefill (prompt processing) stage, which is **compute-bound** ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Previous%20research%20,large%20VRAM%20bandwidth%20to%20accelerate)). KV caching primarily benefits the decode stage; however, it can indirectly aid TTFT when prompts are extremely long. Without caching, if one attempted to generate outputs incrementally with partial prompts, the model would have to re-process earlier prompt tokens repeatedly, delaying the first output. With caching, the prompt can be processed once, cached, and the first token is emitted immediately after, without any redundant recomputation. Moreover, caching enables techniques like **prompt segmentation** or streaming, where very long prompts are handled in chunks – the model can begin generating from an initial chunk while caching context for later chunks, potentially reducing the latency before first response. In high-concurrency settings, KV caching also facilitates **prompt reuse optimizations**. For example, vLLM’s *PagedAttention* system can detect if a new request shares a prefix with a previous request and reuse the cached keys/values for that prefix ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=PagedAttention%20has%20another%20key%20advantage%3A,shared%20between%20the%20output%20sequences)). This means the prompt processing for that portion can be skipped, yielding a faster first token for the new request. In summary, KV caching ensures that *TTFT grows roughly linearly with prompt length*, without additional replay overhead. It “guarantees low latency” even as throughput is increased ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Doubao,%EC%9D%98%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EB%AA%A9%ED%91%9C%EB%A5%BC%20%EB%8F%99%EC%8B%9C%EC%97%90%20%EB%8B%AC%EC%84%B1%ED%95%A9%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%EA%B2%B0%ED%95%A9%ED%95%98%EC%97%AC%20%EB%82%AE%EC%9D%80%20%EC%A7%80%EC%97%B0%20%EC%8B%9C%EA%B0%84%EC%9D%84%20%EB%B3%B4%EC%9E%A5%ED%95%98%EB%A9%B4%EC%84%9C,%EC%9D%98%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EB%AA%A9%ED%91%9C%EB%A5%BC%20%EB%8F%99%EC%8B%9C%EC%97%90%20%EB%8B%AC%EC%84%B1%ED%95%A9%EB%8B%88%EB%8B%A4)), by separating prompt computation from generation.

**Throughput / Time per Output Token (TPOT):** The KV cache’s effect on per-token generation speed is even more pronounced. With caching, each output token only incurs the computation for that token’s forward pass and attention with stored states, rather than reprocessing the entire sequence. This yields a **much lower TPOT (higher token throughput)** than would otherwise be possible. For example, one study noted that **without** caching, generating the 100th token would require recomputing attention for the first 99 tokens, and similarly for the 101st token ([Memory Optimization in LLMs: Leveraging KV Cache Quantization for Efficient Inference | by Tejaswi kashyap | Medium](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=This%20dependency%20on%20sequential%20data,calculations%20for%20the%20100th%20token)). With KV caching, the model instead *directly retrieves* the stored keys/values for the first 99 tokens and only computes attention for the new token ([Memory Optimization in LLMs: Leveraging KV Cache Quantization for Efficient Inference | by Tejaswi kashyap | Medium](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=prediction%20of%20the%20101st%20token,calculations%20for%20the%20100th%20token)). This *streamlines generation*, as the cost per token remains roughly constant (or increases sub-linearly) even as the sequence grows. Quantitatively, caching reduces each layer’s attention from a full $QK^T$ computation over $n$ tokens to a much smaller operation for the new token. The result is that **throughput in tokens/sec stays high** and does not degrade as rapidly with sequence length. In fact, KV caching is considered a *standard practice* precisely because it enables near-constant per-token generation time, which is crucial for long outputs ([Memory Optimization in LLMs: Leveraging KV Cache Quantization for Efficient Inference | by Tejaswi kashyap | Medium](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=The%20key,computations%20for%20the%20100th%20token)) ([Memory Optimization in LLMs: Leveraging KV Cache Quantization for Efficient Inference | by Tejaswi kashyap | Medium](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=them%2C%20thus%20accelerating%20the%20generation,process)). Without KV cache, the time per output token would grow with sequence length, quickly becoming untenable for large $n$. In sum, KV caching is what makes *streaming generation efficient*, yielding low TPOT and high overall throughput for long sequences.

It should be noted that the benefits of KV caching come at the cost of **memory usage**. The cache stores $K$ and $V$ tensors for every generated token across all layers. This means the memory footprint of the KV cache scales linearly with sequence length (and with batch size and number of transformer layers/heads). For a given model, each token contributes a fixed amount of KV data: e.g., for an $H$-dimensional hidden state and $L$ layers, the cache per token might be on the order of $2 \\times L \\times H$ values (two matrices per layer, key and value). Over a long sequence, this can accumulate to tens or even hundreds of gigabytes. While KV caching **reduces compute cost**, it **shifts the bottleneck to memory** in the decode phase (). This has implications for GPU memory capacity and throughput, which we discuss next.

## **On-GPU KV Cache: Memory Efficiency and GPU Utilization**

Keeping the KV cache in **GPU memory (VRAM)** is ideal for performance. GPU HBM (High-Bandwidth Memory) offers extremely fast access (hundreds of GB/s bandwidth) and allows the GPU to perform attention with minimal latency. When the entire cache fits in GPU memory, each new token’s attention can read prior keys/values at full HBM speed ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=PCIe%20bandwidth%20between%20host%20memory,bus%20a%20new%20performance%20bottleneck)). This avoids PCIe transfers or CPU involvement, eliminating I/O bottlenecks. Consequently, in a GPU-only setup, the *decode phase* can often sustain near-maximum throughput until memory capacity is reached. However, the **sheer size** of the KV cache for large models and long sequences can easily exhaust GPU memory. For instance, running a 13B-parameter model at batch size 32 with a 4K token context requires on the order of *100 GB* of KV cache storage – about 4.2× the size of the model’s weights ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=weights.%20For%20instance%2C%20a%20mid,size%20of%20the%20model%20itself)). Another estimate found that for a smaller 8B model generating 1 million tokens (extreme case), the KV cache would be \~128 GB (with bfloat16 precision), dwarfing the \~15 GB model size ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=and%20efficiency,with%2024%20GB%20of%20memory)). Clearly, these sizes far exceed typical single-GPU memory (e.g. 24 GB or 40 GB), indicating that naive GPU-only caching is not feasible for very long contexts or many concurrent sequences.

Moreover, even before hitting absolute capacity limits, **memory allocation inefficiencies** can degrade GPU utilization. Traditional frameworks often pre-allocate large contiguous memory blocks for the maximum possible sequence length per batch, leading to **fragmentation and over-provisioning** ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=,reservation)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Image%20Example%20generation%20process%20for,a%20request%20with%20PagedAttention)). Research from the vLLM project observed that existing systems wasted *60–80% of allocated GPU memory* due to fragmentation and unused space reserved for worst-case sequence lengths ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=,reservation)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Image%20Example%20generation%20process%20for,a%20request%20with%20PagedAttention)). This overallocation means fewer sequences can be batched, leaving GPU compute under-utilized. To address this, vLLM introduced **PagedAttention**, which treats GPU memory like a virtual memory system with granular “pages” for KV storage ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=To%20address%20this%20problem%2C%20we,and%20fetches%20these%20blocks%20efficiently)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Because%20the%20blocks%20do%20not,as%20new%20tokens%20are%20generated)). Instead of requiring each sequence’s cache to be contiguous, PagedAttention **partitions the KV cache into blocks** and fills GPU memory dynamically as needed, akin to an OS paging scheme ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=To%20address%20this%20problem%2C%20we,and%20fetches%20these%20blocks%20efficiently)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Because%20the%20blocks%20do%20not,as%20new%20tokens%20are%20generated)). The result is that memory waste is reduced to \<4%, enabling near-optimal VRAM usage ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Image%20Example%20generation%20process%20for,a%20request%20with%20PagedAttention)). By reclaiming fragmented memory, such an approach allows **more sequences to be served concurrently** or longer sequences to fit, thereby improving overall throughput. Additionally, PagedAttention permits **memory sharing** of common tokens: if multiple sequences share the same prefix, they can point to the same physical KV blocks (copy-on-write) rather than duplicating them ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=PagedAttention%20has%20another%20key%20advantage%3A,shared%20between%20the%20output%20sequences)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=PagedAttention%20naturally%20enables%20memory%20sharing,Write%20mechanism)). This significantly reduces memory overhead in use cases like multi-sample generation (e.g. beam search or sampling multiple continuations from one prompt), improving throughput by up to 2.2× in those scenarios ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=PageAttention%E2%80%99s%20memory%20sharing%20greatly%20reduces,methods%20practical%20in%20LLM%20services)).

Another on-GPU optimization is **KV cache quantization**. While model weights are often compressed to 8-bit or 4-bit to save memory, the KV cache (being a form of *activation*) is typically left in 16-bit for accuracy. However, recent work shows that keys and values can be quantized more aggressively with minimal quality loss. Techniques like **KIVI (Key-Value Vector Quantization)** and **LogQuant** compress the KV cache to 2-bit per element, using clever schemes to preserve attention fidelity ([KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache \- arXiv](https://arxiv.org/abs/2402.02750#:~:text=KIVI%3A%20A%20Tuning,Llama%2C%20Falcon%2C%20and%20Mistral)) ([jy-yuan/KIVI: \[ICML 2024\] KIVI: A Tuning-Free Asymmetric ... \- GitHub](https://github.com/jy-yuan/KIVI#:~:text=jy,friendly%20design%20allows%20LLMs)). For example, KIVI applies per-channel quantization for keys and per-token quantization for values, achieving an 8× reduction in KV size without fine-tuning the model ([Memory Optimization in LLMs: Leveraging KV Cache Quantization ...](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=the%20precision%20of%20its%20parameters%2C,bit)) ([jy-yuan/KIVI: \[ICML 2024\] KIVI: A Tuning-Free Asymmetric ... \- GitHub](https://github.com/jy-yuan/KIVI#:~:text=This%20algorithm%20optimizes%20memory%20usage,friendly%20design%20allows%20LLMs)). Such quantization can shrink a 128 GB KV cache to \~16 GB. This directly boosts GPU utilization: more tokens can be cached on-GPU, avoiding offload. NVIDIA’s FasterTransformer library also supports quantized KV cache (e.g. FP8 keys/values), which **increases the effective cache capacity and hence throughput** ([Quantized KV Cache \- vLLM](https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache.html#:~:text=Quantizing%20the%20KV%20cache%20to,in%20the%20cache%2C%20improving%20throughput)). One caveat is that compressing KV may slightly increase computation per token (for dequantization) and could incur a small accuracy hit, but studies report negligible impact on model quality ([LogQuant: Log-Distributed 2-Bit Quantization of KV Cache ... \- arXiv](https://arxiv.org/abs/2503.19950#:~:text=arXiv%20arxiv,LLM%29%20inference%2C%20delivering)). In practice, KV cache quantization offers a powerful trade-off: *on-GPU memory efficiency* in exchange for a modest increase in arithmetic operations.

When KV cache fits well in GPU memory and is efficiently managed, the decode phase becomes *bandwidth-bound by GPU memory*. Modern GPUs have enormous HBM bandwidth (e.g. an NVIDIA H100 SXM has \~3 TB/s aggregate) ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=27%20TB%2Fs%20,s%29%20per)) ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=Memory%20type%2032x%20Micron%2064GB,0)), which can sustain very high token generation rates. In this regime, GPU compute units might not be fully utilized if each token’s computation is small compared to the cost of streaming data from memory. Nonetheless, by maximizing parallelism (batching multiple sequences) and using high-bandwidth memory, one can keep the GPU busy. A Micron study of Llama-2 70B inference found that increasing batch size yields near-linear throughput gains until the point that **KV cache saturates memory capacity or bandwidth** ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=match%20at%20L487%20the%20KV,more%20than%20increasing)) ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=Consistent%20with%20Figure%204%2C%20Figure,has%20a%20significant%20impact%20on)). If batch size or sequence length grows so large that the KV cache no longer fits in HBM, throughput collapses or inference fails due to OOM ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=match%20at%20L511%20inference%20fails,from%20four%20GPUs%20down%20to)). Therefore, *eliminating memory bottlenecks* on GPU is key to high utilization. Techniques like PagedAttention and quantization, as well as **strategic cache eviction**, ensure the GPU’s memory is used optimally without idle gaps. For instance, **vLLM reports up to 24× higher throughput** than a naive HuggingFace Transformers server by virtue of its optimized KV memory management ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=open,requiring%20any%20model%20architecture%20changes)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Beyond%20State)). This enormous speedup (achieved on the same hardware) underscores that the **bottleneck in LLM serving is often memory management, not raw compute** ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=The%20Secret%20Sauce%3A%20PagedAttention)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=,reservation)). By solving GPU memory inefficiencies, vLLM and similar systems keep more of the GPU’s FLOPs busy on generating tokens instead of waiting on memory or leaving capacity unused.

([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html)) *Throughput of LLM serving with optimized KV cache vs standard implementations.* *In this example (LLaMA model, single GPU), an optimized KV caching engine (blue, vLLM with PagedAttention) achieves over an order-of-magnitude higher throughput than standard HuggingFace Transformers (red) and even surpasses highly optimized frameworks like HuggingFace TGI (orange) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=open,requiring%20any%20model%20architecture%20changes)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=NVIDIA%20A100%20GPU%20%2840GB%29,5x%20higher%20throughput%20than%20TGI)). By efficiently utilizing GPU memory, the KV cache optimization unlocks much greater token output rates.*

In summary, **keeping the KV cache on-GPU and managing it intelligently** yields the best TTFT and TPOT: the first token can be produced as soon as the prompt is processed, and subsequent tokens stream out at the maximal rate the GPU’s memory bandwidth allows. However, GPU memory is a scarce (and expensive) resource. Next, we explore strategies to extend KV caching beyond the GPU when memory limits are reached, and the performance implications of doing so.

## **Extending KV Cache Beyond GPU Memory (CPU Offloading & NVMe)**

When the KV cache cannot be fully kept in GPU VRAM – due to very long sequences, large batches, or limited GPU memory – the typical solution is to **offload** some or all of it to lower-tier memory, such as host (CPU) memory or even disk. Offloading the KV cache increases capacity (enabling longer contexts), but comes at the cost of additional latency and reduced throughput, because the bandwidth and access latencies of CPU memory or SSDs are much lower than on-GPU memory ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=the%20GPU%20begins%20the%20decoding,KV%20cache%20volumes%20between%20the)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=PCIe%20bandwidth%20between%20host%20memory,bus%20a%20new%20performance%20bottleneck)).

**Offloading to CPU Memory (RAM):** Modern inference frameworks like **DeepSpeed-Inference** (ZeRO-Offload) and Hugging Face Accelerate introduced CPU offloading capabilities to handle model states that don’t fit in GPU memory ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=Inference%20%28Aminabadi%20et%20al,Rajbhandari) ) ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=an%20essential%20technique%20%E2%80%94%20as,computation%20and%20miss%20great%20opportunities) ). Initially, these were used to offload model *weights*, but recent systems also allow offloading the KV cache to host memory ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=Modern%20LLM%20serving%20systems%20like,KV%20cache%20a%20significant%20challenge)) ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=Different%20execution%20styles%20of%20Transformer,1)). In an offload scenario, when the cache grows beyond the GPU budget, older key/value tensors (or entire layers’ cache blocks) are copied over the PCIe bus to the CPU’s DRAM, and brought back to the GPU when needed for attention. This **alleviates GPU memory pressure**, enabling much longer sequences – for example, offloading parts of the KV cache to CPU allowed demonstration of **1-million-token generation** on a single 24 GB GPU (with an 8B model) in a recent work ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=significantly%20reducing%20memory%20footprint,without%20approximation%20methods)) ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=Modern%20Large%20Language%20Models%20,requiring%20millions%20of%20tokens)). However, the trade-off is **throughput**. PCIe bandwidth (often 16–32 GB/s per GPU) is an order of magnitude lower than HBM bandwidth ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=offloading%20strategy%20introduces%20severe%20performance,movement%20over%20a%20narrow%20PCIe)), and memory access latency is higher. Thus, each generation step may be delayed waiting for required KV data to transfer from host to device. Even with batched transfers and prefetching, the **decode phase becomes I/O-bound** over PCIe ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=complicated%20host,bus%20a%20new%20performance%20bottleneck)). As InstInfer’s authors note, the memory-bound nature of decoding means frequent large KV transfers “make data movement over a narrow PCIe bus a new performance bottleneck” ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=complicated%20host,bus%20a%20new%20performance%20bottleneck)). In practice, straightforward CPU offloading can dramatically **increase TPOT (per-token time)** – generation might slow to a few tokens per second or less if every token requires gigabytes of retrieval.

Techniques to mitigate this include: (1) **Prefetching** – overlapping computation with data transfer by fetching the next token’s KV from CPU while the current token is being processed ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=To%20enable%20larger%20batch%20sizes,due%20to%20limited%20PCIe%20bandwidth)). This can hide a portion of the latency, though not all (especially if sequence lengths are very large) ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=Even%20with%20conventional%20prefetching%20techniques,This%E2%80%A6)). (2) **Selective Offload** – only moving less frequently used parts of the cache. For example, some systems keep the most recent $K$ tokens’ cache on GPU and offload older ones, since attention probability often decays with distance. (3) **Quantized transfer** – transferring KV in compressed form (e.g. 8-bit or 4-bit) to reduce bandwidth, then decompressing on GPU ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=to%20CPU%20memory%2C%20as%20shown,due%20to%20limited%20PCIe%20bandwidth)). NVIDIA’s offloading approach leverages such quantization to cut data size, but the fundamental PCIe bottleneck remains ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=to%20CPU%20memory%2C%20as%20shown,due%20to%20limited%20PCIe%20bandwidth)). A recent proposal, **InfiniGen**, goes further by *speculating* which past tokens will be most relevant for the next attention step and **only prefetching those** from CPU to GPU ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=generation%2C%20which%20integrates%20seamlessly%20with,based%20LLM%20systems)) ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=Modern%20LLM%20serving%20systems%20like,KV%20cache%20a%20significant%20challenge)). InfiniGen performs a lightweight “rehearsal” of the attention calculation on CPU to identify the top-$k$ keys needed for accurate attention, and fetches just those KV entries ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=which%20scales%20with%20sequence%20length,based)) (). This reduced data transfer can improve throughput by up to 3× in long-context scenarios compared to naive offloading (). Such advanced strategies show that *algorithmic intelligence can partially overcome bandwidth limits*, but they add system complexity and only mitigate, not eliminate, the offloading cost.

**Offloading to SSD/NVMe (Persistent Storage):** In extreme cases where even CPU memory is insufficient (or for cost-saving in throughput-oriented batch jobs), KV cache might be spilled to disk. This is the approach taken by **FlexGen**, a system designed to enable LLM inference on resource-constrained hardware (e.g., a single GPU with limited memory) ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=Recent%20years%20have%20witnessed%20the,such%20commodity%20hardware%2C%20offloading%20is) ) ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=an%20essential%20technique%20%E2%80%94%20as,computation%20and%20miss%20great%20opportunities) ). FlexGen can offload model weights and KV cache to a high-speed NVMe SSD, using 4-bit compression for both ([\[PDF\] FlexGen: High-Throughput Generative Inference of Large Language ...](https://arxiv.org/pdf/2303.06865#:~:text=,Within)). By carefully scheduling data movement and compute, FlexGen demonstrated that a 175B parameter model (OPT-175B) could be run with effective batch size up to 256 on a single GPU \+ SSD, albeit with **very high latency (minutes per output)** ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=more%20than%2040%C3%97%20higher%20throughput,memory%20issues) ) ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=FlexGen%20achieves%2069%C3%97%20higher%20maximum,with%20latency) ). In other words, FlexGen sacrifices responsiveness for the ability to generate large batches or long outputs on minimal hardware – achieving up to *100× higher throughput* than GPU-only methods in the limit of large batch, but with **latencies of thousands of seconds** ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=more%20than%2040%C3%97%20higher%20throughput,memory%20issues) ) ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=FlexGen%20achieves%2069%C3%97%20higher%20maximum,get%02ting%20rid%20of%20disk%20offloading) ). This underscores a key point: **offloading expands what is possible, but usually at a severe performance penalty**. SSDs, even fast NVMe ones, have bandwidth in the few GB/s range and millisecond-scale access latencies, far slower than RAM. Copying KV cache from SSD for each token would be completely impractical if done synchronously. FlexGen instead reorders execution to amortize disk reads, for example by generating multiple tokens in a batch (to increase sequential access) and by overlapping I/O and compute. Still, when comparing to in-VRAM serving, the gap is large: one study showed that when KV cache exceeds GPU memory and spills to NVMe, **throughput drops off sharply** as batch size grows ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Performance%20Degradation%20With%20Offloading,that%20of%20GPU%20memory%2C%20prior)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=sequence%20lengths%20are%20set%20to,exceeds%20the%20available%20GPU%20memory)). The authors observed DeepSpeed offloading faltered beyond batch size 8, and FlexGen beyond batch size 64, for a 13B model with 1024-token sequences – once the KV cache could no longer be fully cached in GPU, performance degraded significantly ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Performance%20Degradation%20With%20Offloading,that%20of%20GPU%20memory%2C%20prior)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=sequence%20lengths%20are%20set%20to,exceeds%20the%20available%20GPU%20memory)). These results highlight that while persistent storage can act as an “overflow” for KV data, using it incurs an **orders-of-magnitude slowdown** in TPOT if not addressed with specialized solutions.

**System Design Considerations:** To make KV offloading viable, researchers have explored architectural innovations. One approach is to bring computation *closer* to the data – for example, **InstInfer** proposes offloading not just KV data but the *attention computation itself* to a smart storage device ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=accesses%20due%20to%20limited%20PCIe,13B%20model%20using%20an%20NVIDIA)). InstInfer uses a **Computational Storage Drive (CSD)** – essentially an NVMe SSD with an embedded FPGA – to perform the attention operation on the KV cache directly in-storage, exploiting the SSD’s internal parallel bandwidth ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=critical%20computation%20%28i,based%20solutions%20such%20as%20FlexGen)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Computational%20Storage%20Drives%20%28CSDs%29%20,which%20is%20significantly%20higher%20than)). Because modern SSDs aggregate many flash chips, their internal bandwidth (tens of GB/s) can far exceed the external PCIe bandwidth (e.g. 36 GB/s) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=SSDs%20,the%20KV%20cache%20bandwidth%20requirements)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=cache%20for%20long,limited)). InstInfer’s in-situ attention engine uses this internal bandwidth to reduce the volume of data that must cross the PCIe bus ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Computational%20Storage%20Drives%20%28CSDs%29%20,which%20is%20significantly%20higher%20than)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=cache%20for%20long,the%20KV%20cache%20bandwidth%20requirements)). Only the resulting attention outputs (much smaller than the raw KV data) are sent to the GPU, dramatically cutting down PCIe traffic. In experiments, this yielded up to **11.1× throughput improvement** for long-sequence inference compared to conventional SSD offloading (FlexGen) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=KV%20transfer%20overheads,based%20solutions%20such%20as%20FlexGen)). The general principle is to **minimize moving the KV cache** over slow links; either keep it on GPU, or if it must reside elsewhere, move compute to that location or only stream the “essentials” needed by the GPU. Another design aspect is ensuring *direct paths* for data: GPU-direct storage (bypassing CPU) or NVLink-connected memory servers can alleviate bottlenecks. Some offloading systems use CUDA-managed memory such that evicted cache pages reside in CPU or NVMe but are fetched on-demand via unified memory mechanisms, simplifying the programming model at the cost of unpredictable latency spikes.

Despite these advances, whenever KV caching extends to host or disk, one must accept some **latency overhead**. This often manifests as a higher TPOT – e.g., instead of 5–10 ms per token on GPU, an offloaded setup might incur 50–100 ms or more per token due to data transfer, significantly reducing tokens/sec. Techniques like speculative decoding have been used to mask this: OpenAI’s **speculative decoding** approach runs a small “draft” model to generate several tokens in advance, so that the larger model can skip ahead and reduce waiting time per token ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Prefill%20FFN%3A%20W4A8%20%EC%96%91%EC%9E%90%ED%99%94%EB%A5%BC%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC,8%EA%B9%8C%EC%A7%80%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%B5%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%EB%B9%84%EC%9C%A8%EC%9D%B4%20%EB%82%AE%EC%9D%80%20%EC%9E%A5%EB%B9%84%20Serving%EC%9D%84%20%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC,%EC%A0%84%EB%9E%B5%EC%9D%84%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC%20TPOT%20%EC%A7%80%ED%91%9C%EB%A5%BC%20%EB%82%AE%EC%B6%A5%EB%8B%88%EB%8B%A4)). This can lower the effective TPOT as seen by the user (the large model doesn’t actually generate every token). The Doubao-1.5 system, for instance, combined speculative decoding with KV cache optimizations to *“reduce the TPOT metric”* while maintaining low latency ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Decode%20%EB%8B%A8%EA%B3%84%3A%20%EA%B3%84%EC%82%B0%20%EB%B3%91%EB%AA%A9%20%ED%98%84%EC%83%81%EC%9D%80,%EC%A0%84%EB%9E%B5%EC%9D%84%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC%20TPOT%20%EC%A7%80%ED%91%9C%EB%A5%BC%20%EB%82%AE%EC%B6%A5%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%EB%B9%84%EC%9C%A8%EC%9D%B4%20%EB%82%AE%EC%9D%80%20%EC%9E%A5%EB%B9%84%20Serving%EC%9D%84%20%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC,%EC%A0%84%EB%9E%B5%EC%9D%84%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC%20TPOT%20%EC%A7%80%ED%91%9C%EB%A5%BC%20%EB%82%AE%EC%B6%A5%EB%8B%88%EB%8B%A4)). In summary, the **role of KV cache in persistent storage** is as a necessary fallback for enabling *ultra-long contexts or low-memory deployments*, and it requires careful system engineering (prefetching, compression, near-data processing) to achieve acceptable throughput. The throughput hit can be mitigated but not entirely eliminated – there is an inherent trade-off between *context length vs. speed*.

## **Evolution in Large-Scale Deployments**

KV caching techniques have evolved significantly as LLM deployments have grown from single-GPU systems to massive multi-node clusters serving thousands of queries. Early LLM deployments (GPT-2 era) had modest context windows (≤1024 tokens) that could easily be cached in GPU memory, and single-digit batch sizes – a straightforward GPU-only KV cache sufficed. As we reached models like GPT-3 and PaLM with contexts of 2048 or 4096, and serving infrastructures that handle many concurrent chats, new challenges emerged. Today, with models like GPT-4 (32K context) (), Claude (100K+ context), and even research prototypes with millions of tokens context ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=Modern%20Large%20Language%20Models%20,requiring%20millions%20of%20tokens)), the KV cache is front-and-center in system design.

**Multi-GPU and Distributed Inference:** In large-scale deployments, models are often parallelized across multiple GPUs (using tensor or pipeline parallelism). KV caching in this scenario is partitioned according to the model parallel scheme. For example, in tensor-parallel Transformers, each GPU holds a slice of the key/value vectors (for the attention heads it is responsible for) – so the KV cache is inherently distributed across GPUs, which can alleviate per-GPU memory usage. However, this requires coordination: when generating a token, either all shards must communicate their attention results or gather keys/queries across nodes. High-speed interconnects (NVLink, InfiniBand) are used to make distributed KV access fast, and caches are often **replicated or swapped** between nodes to balance load ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,replication%20for%20enhanced%20fault%20tolerance)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=2.%20P%20er,replication%20for%20enhanced%20fault%20tolerance)). Some inference serving systems split the workload into a **prefill stage and a decode stage on separate hardware pools** ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Decode%20%EB%8B%A8%EA%B3%84%3A%20%EA%B3%84%EC%82%B0%20%EB%B3%91%EB%AA%A9%20%ED%98%84%EC%83%81%EC%9D%80,%EC%A0%84%EB%9E%B5%EC%9D%84%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC%20TPOT%20%EC%A7%80%ED%91%9C%EB%A5%BC%20%EB%82%AE%EC%B6%A5%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%EC%A0%84%EB%B0%98%EC%A0%81%EC%9C%BC%EB%A1%9C%20PD%28Prefill,%EB%8B%A4%EC%9D%8C%EA%B3%BC%20%EA%B0%99%EC%9D%80%20%EC%B5%9C%EC%A0%81%ED%99%94%EB%A5%BC%20%EA%B5%AC%ED%98%84%ED%96%88%EC%8A%B5%EB%8B%88%EB%8B%A4)). In such designs, one set of GPUs handles prompt processing and then transfers the KV cache (or the last layer’s activations) to another set of GPUs specialized for fast decoding. This *disaggregation* can improve utilization – e.g., the prefill GPUs (which are compute-bound) can be fully loaded with new prompts while the decode GPUs (memory-bandwidth-bound) continuously generate tokens ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=Prefill%20FFN%3A%20W4A8%20%EC%96%91%EC%9E%90%ED%99%94%EB%A5%BC%20%EC%B1%84%ED%83%9D%ED%95%98%EC%97%AC,8%EA%B9%8C%EC%A7%80%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%B5%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%EC%A0%84%EB%B0%98%EC%A0%81%EC%9C%BC%EB%A1%9C%20PD%28Prefill,%EB%8B%A4%EC%9D%8C%EA%B3%BC%20%EA%B0%99%EC%9D%80%20%EC%B5%9C%EC%A0%81%ED%99%94%EB%A5%BC%20%EA%B5%AC%ED%98%84%ED%96%88%EC%8A%B5%EB%8B%88%EB%8B%A4)). The KV cache transfer between stages becomes a critical path; optimizations like zero-copy RDMA transfers, pipeline parallel network I/O, and compression are applied to minimize overhead ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%EC%A0%84%EB%B0%98%EC%A0%81%EC%9C%BC%EB%A1%9C%20PD%28Prefill,%EB%8B%A4%EC%9D%8C%EA%B3%BC%20%EA%B0%99%EC%9D%80%20%EC%B5%9C%EC%A0%81%ED%99%94%EB%A5%BC%20%EA%B5%AC%ED%98%84%ED%96%88%EC%8A%B5%EB%8B%88%EB%8B%A4)) ([Doubao 1.5 pro, 새로운 중국의 모델(AIME o1 뛰어넘음) \- 특이점이 온다 마이너 갤러리](https://m.dcinside.com/board/thesingularity/617126?recommend=1#:~:text=%ED%85%90%EC%84%9C%20%EC%A0%84%EC%86%A1%EC%9D%84%20%EC%9C%84%ED%95%B4%20%EB%A7%9E%EC%B6%A4%ED%99%94%EB%90%9C%20RPC,KV%20%EC%BA%90%EC%8B%9C%20%EC%A0%84%EC%86%A1%20%ED%9A%A8%EC%9C%A8%EC%84%B1%EC%9D%84%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%B5%EB%8B%88%EB%8B%A4)). By tuning the ratio of prefill to decode instances and using autoscaling, large deployments ensure that neither stage becomes a bottleneck, thereby optimizing both TTFT and throughput collectively.

**Memory Pooling and KV Swapping:** Serving many concurrent sessions (e.g. a cloud LLM API) often leads to **GPU memory over-provisioning for KV** – if we naively allocate the maximum cache for each session, much of it sits idle (since not all sessions will hit the max length simultaneously) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=2)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=Most%20frameworks%20preallocate%20KV%20cache,This%20results%20in%20inefficient%E2%80%A6)). Solutions like **vLLM** address this by treating the GPU KV memory as a global pool that can be dynamically allocated to active generation streams, rather than fixed per-session blocks ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Because%20the%20blocks%20do%20not,as%20new%20tokens%20are%20generated)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Image%20Example%20generation%20process%20for,a%20request%20with%20PagedAttention)). When one session finishes or pauses, its KV cache space can be reclaimed for others. Furthermore, if more sessions are active than GPU memory can hold, a system can **swap out** the KV cache of inactive sessions to host memory. The *DéjàVu* serving system (2024) uses *per-microbatch KV cache swapping* – when a given request is not currently producing a token (e.g., waiting for its turn in a batch), its KV data may be moved to CPU, and then swapped back in when needed ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,replication%20for%20enhanced%20fault%20tolerance)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,replication%20for%20enhanced%20fault%20tolerance)). This effectively treats the GPU like a small cache and the CPU memory as backing store, similar to an OS paging out a process’s memory when it’s not running. By doing this opportunistically between generation “microbatches”, DéjàVu avoids having to allocate full KV space for every concurrent request, greatly **improving hardware utilization** ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,of%20failures%2C%20reducing%20overall%20throughput)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,replication%20for%20enhanced%20fault%20tolerance)). The trade-off is some added latency when a swapped-out session resumes (to swap its KV in), but if done cleverly during idle times, the impact on latency can be minimal. This technique, combined with *scheduling algorithms to fill pipeline bubbles*, allowed DéjàVu to reduce GPU idle time and serve more requests with the same hardware ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=due%20to%20three%20main%20challenges%3A)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=Prompt%20processing%20can%20be%20significantly,causing%20GPU%20idling%20and%20inefficiencies)).

**Fault Tolerance and Persistence:** In large deployments, maintaining service continuity is crucial. KV cache data, however, traditionally lives in volatile GPU/CPU memory and is lost if a server fails or a process restarts, interrupting any in-progress generation. Emerging systems therefore consider **persisting KV cache to durable storage** not just for overflow, but for fault tolerance. For example, DéjàVu implements *KV cache state replication* – periodically writing copies of KV cache to persistent storage or another node, so that if a generating process crashes, a backup can recover and resume the generation from the last saved state ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,replication%20for%20enhanced%20fault%20tolerance)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=2.%20P%20er,replication%20for%20enhanced%20fault%20tolerance)). This kind of **persistent KV store** for LLMs is akin to a checkpoint/restart mechanism at the inference level. While not yet common, it points to an evolution where KV caches might be treated as long-lived data (especially for long conversations or document streams) that can survive across sessions or be handed off between servers. Such designs will require fast serialization of large tensors and perhaps standardized formats for cache interoperability.

Over the past few years, we’ve thus seen the KV cache transition from a simple GPU memory resident array to a **multi-tier hierarchical cache** spanning GPU HBM, CPU RAM, and SSD – managed much like memory hierarchies in computer systems. **On-GPU caching** is the fastest tier, optimized with paging and quantization. **Offloading to CPU** provides a larger but slower tier, improved by prefetch and partial compute offload (InfiniGen, HeadInfer). **Extending to SSD** offers massive capacity at further reduced speed, spurring innovations like InstInfer’s compute-in-storage to bridge the gap. In parallel, **software frameworks** evolved: from basic PyTorch generation to specialized engines like FasterTransformer (NVIDIA), TensorRT-LLM, vLLM, and others, all focused on maximizing throughput by handling KV cache smartly. The impact is evident in metrics – for example, Meta’s research showed that with efficient caching and memory management, a **70B model’s throughput scales almost linearly with batch size** until the KV cache hits memory limits ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=the%20KV%20cache%20requirement%20is,more%20than%20increasing)) ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=match%20at%20L511%20inference%20fails,from%20four%20GPUs%20down%20to)). Conversely, without such techniques, performance would plateau or crash much sooner.

Looking forward, the trend is towards **treating KV cache as a first-class citizen in system design**. Techniques from classic computer architecture (paging, NUMA-aware placement, prefetching algorithms) and storage systems are being applied to LLM inference. The goal is to combine the *capacity of lower tiers* with the *speed of upper tiers* as much as possible. For instance, one could imagine a future GPU with on-package HBM for current KV and slower off-chip memory for extended KV, managed by hardware. Indeed, hardware vendors are exploring HBM expansions and smarter memory controllers to handle the growing KV sizes ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=and%20KV%20cache%20operations%20in,innovative%20memory%20portfolio%20of%20high)) ([Analyzing LLM performance: The impact of high-bandwidth memory on model inference](https://www.micron.com/content/dam/micron/global/public/documents/products/product-flyer/llm-inference-engineering-report.pdf#:~:text=the%20KV%20cache%20requirement%20is,more%20than%20increasing)). Until then, software will continue to innovate: recent ICML 2024 work (HeadInfer) showed that by offloading less-critical attention heads to CPU and only keeping one head’s KV on GPU, one can achieve *4 million token* context on a 24 GB GPU ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=significantly%20reducing%20memory%20footprint,without%20approximation%20methods)) ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=Modern%20Large%20Language%20Models%20,requiring%20millions%20of%20tokens)) – an extreme example of trading model capacity for context length. Such approaches, alongside distributed inference networks (e.g. *Petals* which serves models collaboratively over the internet), indicate that **scalability of KV cache is the key to scaling context length**.

## **Conclusion**

The KV cache is a linchpin of modern LLM inference, enabling high-speed generation by avoiding redundant computation. Its benefits for latency (TTFT) and throughput (TPOT) are undeniable – without KV caching, large-scale chatbots and assistants would be impractically slow. By storing past keys and values, models achieve near-constant-time per token generation, turning the challenge of long contexts from a computational problem into a memory management problem. We have surveyed how leading research and engineering efforts have met this challenge: from optimizing on-GPU memory use through paging and quantization, to leveraging CPU and disk to extend context lengths far beyond GPU capacity. In doing so, we saw a clear **evolution of KV cache strategies**: initially all on GPU, then spilling to CPU with careful scheduling, then onto SSD with compression, and even performing attention computation at the storage layer to mitigate I/O bottlenecks. Large-scale LLM services today implement multi-tier caching, dynamic swapping, and even persistence to ensure that GPUs are utilized to their fullest and that users experience both low latency and the ability to handle long or multiple queries.

In academic literature and benchmarks, these optimizations have translated to significant gains – e.g. \>10× throughput improvements ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=KV%20transfer%20overheads,based%20solutions%20such%20as%20FlexGen)) and the practical feasibility of contexts that are orders of magnitude longer than what naive setups could handle ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=significantly%20reducing%20memory%20footprint,without%20approximation%20methods)). However, each step down the memory hierarchy comes with throughput trade-offs, so the optimal design often combines approaches: keeping the working set of KV in fast memory, intelligently prefetching or recomputing what doesn’t fit, and compressing or ignoring what isn’t crucial. **System designers must balance latency vs. cost vs. throughput**: for interactive use, one might favor larger GPUs to keep KV on-device (thus minimizing TPOT), whereas for offline batch processing of very long documents, an NVMe-backed approach might be acceptable.

In closing, as context windows and deployment scales continue to grow, the importance of efficient KV caching will only increase. Techniques like **grouped-query attention and sparse attention** are being explored to inherently reduce KV size or access frequency ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=Sparse%20Attention,overhead%20of%20the%20KV%20cache)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=50%20%2C%20%2071%2C%2056,However%2C%20the)), complementing caching strategies. The state-of-the-art is trending toward *unifying memory and compute*: treating the KV cache like a flexible store that the system can manage akin to an operating system managing RAM and disk. By doing so, future LLM inference systems will deliver the best of both worlds – the **fast response** users expect and the **ability to handle massive contexts** – thereby unlocking more powerful applications of large language models.

**References:**

1. Minghuan Tan et al., “vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.” *UC Berkeley*, 2023\. ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=open,requiring%20any%20model%20architecture%20changes)) ([vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog](https://blog.vllm.ai/2023/06/20/vllm.html#:~:text=Image%20Example%20generation%20process%20for,a%20request%20with%20PagedAttention))

2. Xiurui Pan et al., “InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference.” *arXiv preprint 2409.04992*, 2024\. ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=the%20GPU%20begins%20the%20decoding,KV%20cache%20volumes%20between%20the)) ([InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference](https://arxiv.org/html/2409.04992v1#:~:text=KV%20transfer%20overheads,based%20solutions%20such%20as%20FlexGen))

3. Tejaswi Kashyap, “Memory Optimization in LLMs: KV Cache Quantization for Efficient Inference.” *Medium*, 2024\. ([Memory Optimization in LLMs: Leveraging KV Cache Quantization for Efficient Inference | by Tejaswi kashyap | Medium](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=prediction%20of%20the%20101st%20token,calculations%20for%20the%20100th%20token)) ([Memory Optimization in LLMs: Leveraging KV Cache Quantization ...](https://medium.com/@tejaswi_kashyap/memory-optimization-in-llms-leveraging-kv-cache-quantization-for-efficient-inference-94bc3df5faef#:~:text=the%20precision%20of%20its%20parameters%2C,bit))

4. Lijun Sheng et al., “FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU.” *arXiv preprint 2303.06865*, 2023\. ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=more%20than%2040%C3%97%20higher%20throughput,memory%20issues) ) ( [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/pdf/2303.06865#:~:text=FlexGen%20achieves%2069%C3%97%20higher%20maximum,with%20latency) )

5. Megha Agarwal et al., “LLM Inference Performance Engineering: Best Practices.” *Databricks Engineering Blog*, 2023\. ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=1,450%20words%20per)) ([LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices#:~:text=This%20metric%20is%20driven%20by,number%20of%20tokens%20to%20be))

6. Yihan Dong et al., “HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading.” *arXiv preprint 2502.12574*, 2024\. ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=significantly%20reducing%20memory%20footprint,without%20approximation%20methods)) ([HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading](https://arxiv.org/html/2502.12574v1#:~:text=and%20efficiency,with%2024%20GB%20of%20memory))

7. Don Moon, “LLM Inference Optimization – KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving (DéjàVu).” *Byte-Sized AI Blog*, 2024\. ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=1,replication%20for%20enhanced%20fault%20tolerance)) ([LLM Inference Optimization — KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving, Dejavu | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-kv-cache-streaming-for-fast-fault-tolerant-generative-llm-serving-dejavu-100a93be1679#:~:text=2))

8. Pytorch NVIDIA, “Metrics – NVIDIA NeMo Inference.” *NVIDIA Documentation*, 2023\. ([Metrics — NVIDIA NIM LLMs Benchmarking](https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html#:~:text=Inter,TPOT))

9. Nebius Engineering, “Beyond ChatGPT: Cost and Power of Open-Source LLMs.” *AI Business*, 2025\. ([Beyond ChatGPT: Exploring the Cost and Power of Open-Source LLMs](https://aibusiness.com/generative-ai/beyond-chatgpt-exploring-the-cost-and-power-of-open-source-llms#:~:text=How%20fast%20do%20you%20need,even%20be%20a%20major%20priority)) ([Beyond ChatGPT: Exploring the Cost and Power of Open-Source LLMs](https://aibusiness.com/generative-ai/beyond-chatgpt-exploring-the-cost-and-power-of-open-source-llms#:~:text=What%20is%20the%20cost%20per,input%20tokens%20per%20output%20token))

10. Jaehyun Lee et al., “InfiniGen: Efficient Generative Inference of LLMs via Dynamic KV Cache Management.” *OSDI 2024*. ([LLM Inference: Accelerating Long Context Generation with KV Cache Offloading to CPU Memory, Using InfiniGen | by Don Moon | Byte-Sized AI | Medium](https://medium.com/byte-sized-ai/llm-inference-optimization-accelerating-long-context-generation-with-kv-cache-offloading-to-cpu-12d3bea407d8#:~:text=memory%20requirements%20of%20the%20transient,based)) ()

